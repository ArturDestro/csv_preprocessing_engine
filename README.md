# CSV Preprocessing Engine

A modular, config-driven preprocessing engine for CSV datasets, designed for reproducible and scalable machine learning pipelines.

---

## ğŸš€ Overview

This project provides a **lightweight preprocessing framework** for tabular data stored in CSV files.  
Instead of writing ad-hoc pandas scripts, you define a **configuration-driven pipeline** that transforms raw CSV data into **ML-ready datasets**.

The engine follows clear responsibilities and explicit transformation steps, inspired by production-grade ML pipelines.

---

## âœ¨ Key Features

- ğŸ“„ CSV in â†’ CSV out (no API or frontend required)
- âš™ï¸ Fully **config-driven** preprocessing
- ğŸ§© Modular architecture (Loader, Cleaner, Encoder, Scaler)
- ğŸ” Reproducible transformations (`fit` / `transform`)
- ğŸ§ª Easy to test and extend
- ğŸ§  Designed for ML and data science workflows

---

## ğŸ§± Architecture

The engine is composed of independent modules, each with a **single responsibility**:

```text
engine/
â”œâ”€â”€ loaders.py     # Load CSV files (encoding, separator, path)
â”œâ”€â”€ cleaners.py    # Handle missing or invalid data
â”œâ”€â”€ encoders.py    # Encode categorical features
â”œâ”€â”€ scalers.py     # Scale numerical features
â”œâ”€â”€ pipeline.py    # Orchestrates the preprocessing flow
```
Each component respects the following principles:

No hidden decisions

No dataset-specific logic

Behavior is defined only by configuration

ğŸ”„ Processing Flow
CSV
 â†“
Loader      â†’ reads raw data
 â†“
Cleaner     â†’ handles missing values
 â†“
Encoder     â†’ encodes categorical columns
 â†“
Scaler      â†’ scales numerical features
 â†“
Processed CSV (ML-ready)


âš™ï¸ Configuration Example
config = {
    "loader": {
        "path": "data/raw.csv",
        "separator": ",",
        "encoding": "utf-8"
    },
    "cleaner": {
        "strategy": "fill",
        "method": "mean",
        "columns": ["age", "salary"]
    },
    "encoder": {
        "type": "onehot",
        "columns": ["city", "gender"]
    },
    "scaler": {
        "type": "standard",
        "columns": ["age", "salary"]
    }
}


ğŸ§ª Example Usage
from pipeline import Pipeline

pipeline = Pipeline(config)
df_processed = pipeline.run()
